#include "object_detection_ssd.h"

ObjectDetectionSSD::ObjectDetectionSSD()
{
    is_new_image = false;
    is_first_image = true;
}

ObjectDetectionSSD::~ObjectDetectionSSD()
{
}

void ObjectDetectionSSD::initInferenceEngine()
{
    std::cout << "InferenceEngine: " << GetInferenceEngineVersion() << std::endl;
    // --------------------------- 1. Load inference engine -------------------------------------
    ROS_INFO("Loading Inference Engine");

    ROS_INFO("Device info: ");
    std::cout << ie.GetVersions(device);

    /** Load extensions for the plugin **/

#ifdef WITH_EXTENSIONS
    /** Loading default extensions **/
    if (device.find("CPU") != std::string::npos) {
        /**
         * cpu_extensions library is compiled from "extension" folder containing
         * custom MKLDNNPlugin layer implementations. These layers are not supported
         * by mkldnn, but they can be useful for inferring custom topologies.
        **/
        ie.AddExtension(std::make_shared<Extensions::Cpu::CpuExtensions>(), "CPU");
    }
#endif

    /** Per layer metrics **/
    if (bool_pc) {
        ie.SetConfig({ { PluginConfigParams::KEY_PERF_COUNT, PluginConfigParams::YES } });
    }
    // -----------------------------------------------------------------------------------------------------

    // --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
    ROS_INFO("Loading network files");
    CNNNetReader netReader;
    /** Read network model **/
    netReader.ReadNetwork(model_path);
    /** Set batch size to 1 **/
    ROS_INFO("Batch size is forced to  1.");
    netReader.getNetwork().setBatchSize(1);
    /** Extract model name and load it's weights **/
    std::string binFileName = model_path.substr(0, model_path.find(".")) + ".bin";
    netReader.ReadWeights(binFileName);
    /** Read labels (if any)**/
    std::string labelFileName = model_path.substr(0, model_path.find(".")) + ".labels";
    std::ifstream inputFile(labelFileName);
    std::copy(std::istream_iterator<std::string>(inputFile),
                std::istream_iterator<std::string>(),
                std::back_inserter(labels));
    // -----------------------------------------------------------------------------------------------------

    /** SSD-based network should have one input and one output **/
    // --------------------------- 3. Configure input & output ---------------------------------------------
    // --------------------------- Prepare input blobs -----------------------------------------------------
    ROS_INFO("Checking that the inputs are as the demo expects");
    InputsDataMap inputInfo(netReader.getNetwork().getInputsInfo());

    size_t netInputHeight, netInputWidth;

    for (const auto & inputInfoItem : inputInfo) {
        if (inputInfoItem.second->getTensorDesc().getDims().size() == 4) {  // first input contains images
            imageInputName = inputInfoItem.first;
            inputInfoItem.second->setPrecision(Precision::U8);
            if (bool_auto_resize) {
                inputInfoItem.second->getPreProcess().setResizeAlgorithm(ResizeAlgorithm::RESIZE_BILINEAR);
                inputInfoItem.second->getInputData()->setLayout(Layout::NHWC);
            } else {
                inputInfoItem.second->getInputData()->setLayout(Layout::NCHW);
            }
            const TensorDesc& inputDesc = inputInfoItem.second->getTensorDesc();
            netInputHeight = getTensorHeight(inputDesc);
            netInputWidth = getTensorWidth(inputDesc);
        } else if (inputInfoItem.second->getTensorDesc().getDims().size() == 2) {  // second input contains image info
            imageInfoInputName = inputInfoItem.first;
            inputInfoItem.second->setPrecision(Precision::FP32);
        } else {
            throw std::logic_error("Unsupported " +
                                    std::to_string(inputInfoItem.second->getTensorDesc().getDims().size()) + "D "
                                    "input layer '" + inputInfoItem.first + "'. "
                                    "Only 2D and 4D input layers are supported");
        }
    }

    // --------------------------- Prepare output blobs -----------------------------------------------------
    ROS_INFO("Checking that the outputs are as the demo expects");
    OutputsDataMap outputInfo(netReader.getNetwork().getOutputsInfo());
    if (outputInfo.size() != 1) {
        throw std::logic_error("This demo accepts networks having only one output");
    }
    DataPtr& output = outputInfo.begin()->second;
    outputName = outputInfo.begin()->first;
    const int num_classes = netReader.getNetwork().getLayerByName(outputName.c_str())->GetParamAsInt("num_classes");
    if (static_cast<int>(labels.size()) != num_classes) {
        if (static_cast<int>(labels.size()) == (num_classes - 1))  // if network assumes default "background" class, having no label
            labels.insert(labels.begin(), "fake");
        else
            labels.clear();
    }
    outputDims = output->getTensorDesc().getDims();
    maxProposalCount = outputDims[2];
    objectSize = outputDims[3];
    if (objectSize != 7) {
        throw std::logic_error("Output should have 7 as a last dimension");
    }
    if (outputDims.size() != 4) {
        throw std::logic_error("Incorrect output dimensions for SSD");
    }
    output->setPrecision(Precision::FP32);
    output->setLayout(Layout::NCHW);
    // -----------------------------------------------------------------------------------------------------

    // --------------------------- 4. Loading model to the device ------------------------------------------
    ROS_INFO("Loading model to the device");
    ExecutableNetwork network = ie.LoadNetwork(netReader.getNetwork(), device);
    // -----------------------------------------------------------------------------------------------------

    // --------------------------- 5. Create infer request -------------------------------------------------
    ROS_INFO("Create infer request");
    async_infer_request_curr = network.CreateInferRequestPtr();
    async_infer_request_next = network.CreateInferRequestPtr();

    /* it's enough just to set image info input (if used in the model) only once */
    if (!imageInfoInputName.empty()) {
        auto setImgInfoBlob = [&](const InferRequest::Ptr &inferReq) {
            auto blob = inferReq->GetBlob(imageInfoInputName);
            auto data = blob->buffer().as<PrecisionTrait<Precision::FP32>::value_type *>();
            data[0] = static_cast<float>(netInputHeight);  // height
            data[1] = static_cast<float>(netInputWidth);  // width
            data[2] = 1;
        };
        setImgInfoBlob(async_infer_request_curr);
        setImgInfoBlob(async_infer_request_next);
    }
}

void ObjectDetectionSSD::run()
{
    ros::NodeHandle nh_private("~");

    getParameters(nh_private);

    initROSInterface();

	initInferenceEngine();

    ROS_INFO("Start inference ");

    typedef std::chrono::duration<double, std::ratio<1, 1000>> ms;
    auto total_t0 = std::chrono::high_resolution_clock::now();
    auto wallclock = std::chrono::high_resolution_clock::now();
    double ocv_decode_time = 0, ocv_render_time = 0;

	while(ros::ok())
	{
        ros::spinOnce();

        auto t0 = std::chrono::high_resolution_clock::now();

        // Capture frame
        if (image.empty())
        {
            ROS_WARN("No data received in CameraTopic instance");
            ros::Duration(0.5).sleep();
            continue;
        }

        // Current image is old
        if (!is_new_image)
            continue;

        next_frame = image;
        frameToBlob(next_frame, async_infer_request_next, imageInputName);

        auto t1 = std::chrono::high_resolution_clock::now();
        ocv_decode_time = std::chrono::duration_cast<ms>(t1 - t0).count();

        t0 = std::chrono::high_resolution_clock::now();
        // Main sync point:
        // in the truly Async mode we start the NEXT infer request, while waiting for the CURRENT to complete
        async_infer_request_next->StartAsync();

        // Create objects
        ros_vino::Objects objs_msg;

        if (OK == async_infer_request_curr->Wait(IInferRequest::WaitMode::RESULT_READY)) {
            t1 = std::chrono::high_resolution_clock::now();
            ms detection = std::chrono::duration_cast<ms>(t1 - t0);

            t0 = std::chrono::high_resolution_clock::now();
            ms wall = std::chrono::duration_cast<ms>(t0 - wallclock);
            wallclock = t0;

            t0 = std::chrono::high_resolution_clock::now();
            std::ostringstream out;
            out << "OpenCV cap/render time: " << std::fixed << std::setprecision(2)
                << (ocv_decode_time + ocv_render_time) << " ms";
            cv::putText(curr_frame, out.str(), cv::Point2f(0, 25), cv::FONT_HERSHEY_TRIPLEX, 0.6, cv::Scalar(0, 255, 0));
            out.str("");
            out << "Wallclock time (TRUE ASYNC): ";
            out << std::fixed << std::setprecision(2) << wall.count() << " ms (" << 1000.f / wall.count() << " fps)";
            cv::putText(curr_frame, out.str(), cv::Point2f(0, 50), cv::FONT_HERSHEY_TRIPLEX, 0.6, cv::Scalar(0, 0, 255));

            // ---------------------------Process output blobs--------------------------------------------------
            // Processing results of the CURRENT request
            const float *detections = async_infer_request_curr->GetBlob(outputName)->buffer().as<PrecisionTrait<Precision::FP32>::value_type*>();
            for (int i = 0; i < maxProposalCount; i++) {
                float image_id = detections[i * objectSize + 0];
                if (image_id < 0) {
                    break;
                }

                float confidence = detections[i * objectSize + 2];
                auto label = static_cast<int>(detections[i * objectSize + 1]);
                float xmin = detections[i * objectSize + 3] * width;
                float ymin = detections[i * objectSize + 4] * height;
                float xmax = detections[i * objectSize + 5] * width;
                float ymax = detections[i * objectSize + 6] * height;

                if (bool_raw) {
                    std::cout << "[" << i << "," << label << "] element, prob = " << confidence <<
                                "    (" << xmin << "," << ymin << ")-(" << xmax << "," << ymax << ")"
                                << ((confidence > score_threshold) ? " WILL BE RENDERED!" : "") << std::endl;
                }

                if (confidence > score_threshold) {
                    /** Drawing only objects when > confidence_threshold probability **/
                    std::ostringstream conf;
                    conf << ":" << std::fixed << std::setprecision(3) << confidence;
                    cv::putText(curr_frame,
                                (static_cast<size_t>(label) < labels.size() ?
                                labels[label] : std::string("label #") + std::to_string(label)) + conf.str(),
                                cv::Point2f(xmin, ymin - 5), cv::FONT_HERSHEY_COMPLEX_SMALL, 1,
                                cv::Scalar(0, 0, 255));
                    cv::rectangle(curr_frame, cv::Point2f(xmin, ymin), cv::Point2f(xmax, ymax), cv::Scalar(0, 0, 255));

                    // Add object into objs_msg
                    ros_vino::Object obj_msg;
                    obj_msg.confidence = confidence;
                    obj_msg.label = (static_cast<size_t>(label) < labels.size() ?
                                labels[label] : std::string("label #") + std::to_string(label));
                    obj_msg.x = xmin;
                    obj_msg.y = ymin;
                    obj_msg.height = xmax - xmin;
                    obj_msg.width = ymax - ymin;
                    objs_msg.objects.push_back(obj_msg);
                }
            }
        }

        t1 = std::chrono::high_resolution_clock::now();
        ocv_render_time = std::chrono::duration_cast<ms>(t1 - t0).count();

        // ROS publish result images
        std_msgs::Header header = getHeader();
        sensor_msgs::ImagePtr img_msg = cv_bridge::CvImage(std_msgs::Header(), "bgr8", curr_frame).toImageMsg();
        img_msg->header = header;
        pub_image_rects.publish(*img_msg);

        // ROS publish object list
        objs_msg.header = header;
        pub_objects.publish(objs_msg);

        // Final point:
        // in the truly Async mode we swap the NEXT and CURRENT requests for the next iteration
        curr_frame = next_frame;
        next_frame = cv::Mat();
        async_infer_request_curr.swap(async_infer_request_next);
    
    is_new_image = false;

    }

    auto total_t1 = std::chrono::high_resolution_clock::now();
    ms total = std::chrono::duration_cast<ms>(total_t1 - total_t0);
    std::cout << "Total Inference time: " << total.count() << std::endl;

    /** Show performace results **/
    if (bool_pc) {
        printPerformanceCounts(*async_infer_request_curr, std::cout, getFullDeviceName(ie, device));
    }

}

bool ObjectDetectionSSD::getParameters(ros::NodeHandle nh_private)
{
    ROS_INFO("[ObjectDetectionSSD] Getting parameters ...");

    // ROS parameters
	nh_private.param("topic_image_input", topic_image_input, std::string("/camera/color/image_raw"));
    ROS_INFO("Set topic_image_input to: %s", topic_image_input.c_str());
	nh_private.param("topic_image_output", topic_image_output, std::string("/camera/image_rects"));
    ROS_INFO("Set topic_image_output to: %s", topic_image_output.c_str());
    nh_private.param("topic_objects_output", topic_objects_output, std::string("/camera/detected_objects"));
    ROS_INFO("Set topic_objects_output to: %s", topic_objects_output.c_str());
    
    // Inference Engine's parameters
    std::string path = ros::package::getPath("ros_vino");
    nh_private.param("model_path", model_path, std::string(path + "/models/FP16/mobilenet-ssd/mobilenet-ssd.xml"));
    ROS_INFO("Set model_path to: %s", model_path.c_str());
    nh_private.param("device", device, std::string("CPU"));
    ROS_INFO("Set device to: %s", device.c_str());
    nh_private.param("score_threshold", score_threshold, 0.8);
    ROS_INFO("Set score_threshold to: %f", score_threshold);
    nh_private.param("bool_auto_resize", bool_auto_resize, true);
    ROS_INFO("Set bool_auto_resize to: %d", bool_auto_resize);
    nh_private.param("bool_pc", bool_pc, false);
    ROS_INFO("Set bool_pc to: %d", bool_pc);
    nh_private.param("bool_raw", bool_raw, false);
    ROS_INFO("Set bool_raw to: %d", bool_raw);
    
	return true;
}

void ObjectDetectionSSD::initROSInterface()
{
    ROS_INFO("Publishing to %s", topic_image_output.c_str());
    pub_image_rects = nh.advertise<sensor_msgs::Image>(topic_image_output, 1);

    ROS_INFO("Publishing to %s", topic_objects_output.c_str());
    pub_objects = nh.advertise<ros_vino::Objects>(topic_objects_output, 1);
    
    ROS_INFO("Subscribing to %s", topic_image_input.c_str());
	sub_image_rgb = nh.subscribe(topic_image_input, 1, &ObjectDetectionSSD::rosCallbackImage, this);
}

void ObjectDetectionSSD::rosCallbackImage(const sensor_msgs::Image::ConstPtr& image_msg)
{
    if (is_first_image)
    {
        curr_frame = cv_bridge::toCvCopy(image_msg, "bgr8")->image;
        is_first_image = false;
        // @todo: Validate first input image
        width = (size_t)curr_frame.size().width;
        height = (size_t)curr_frame.size().height;
    }
    else
    {
        image = cv_bridge::toCvCopy(image_msg, "bgr8")->image;
        is_new_image = true;
    }
}

/*---------------------------- Helper Functions ----------------------------*/

void ObjectDetectionSSD::frameToBlob(const cv::Mat& frame, InferRequest::Ptr& inferRequest, const std::string& inputName)
{
    if (bool_auto_resize) {
        /* Just set input blob containing read image. Resize and layout conversion will be done automatically */
        inferRequest->SetBlob(inputName, wrapMat2Blob(frame));
    } else {
        /* Resize and copy data from the image to the input blob */
        Blob::Ptr frameBlob = inferRequest->GetBlob(inputName);
        matU8ToBlob<uint8_t>(frame, frameBlob);
    }
}

std_msgs::Header ObjectDetectionSSD::getHeader()
{
  std_msgs::Header header;
  header.frame_id = "default_camera";

  std::chrono::high_resolution_clock::time_point tp = std::chrono::high_resolution_clock::now();
  int64 ns = tp.time_since_epoch().count();
  header.stamp.sec = ns / 1000000000;
  header.stamp.nsec = ns % 1000000000;
  return header;
}